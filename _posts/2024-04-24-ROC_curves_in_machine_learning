# **ROC curves in machine learning.**

## **What is the ROC curve?**

**Receiver Operating Characteristic (ROC)** curves are graphs that show the performance of a classifier by plotting the true positive and false positive rates. The area under the ROC curve (AUC) measures the performance of a machine learning algorithm. ROC curves visualize the statistical accuracy of classifier selection.

## **Common categorization metrics for ROC curves in machine learning.**
- ### Specificity: A specific index of specificity, representing the ratio of the number of samples identified as negative by the model to the total number of negative samples.        

- ### **Confusion Matrix:** Confusion matrices are visualization tools used especially for supervised learning, in unsupervised learning they are generally called matching matrices. In image accuracy evaluation, it is mainly used to compare the classification results with the actual measured values, and the accuracy of the classification results can be displayed inside a confusion matrix.             
    The structure is shown below:        
    [![Confusion matrix](/master/images/Confusion_matrix.jpg)](https://blog.csdn.net/seagal890/article/details/105059498)          
    Figure 1: Confusion Matrix (image credit: <https://blog.csdn.net/seagal890/article/details/105059498>)                                                                
    The parameters in the image are respectively:                           
    > **True Positive (TP):** The true class of the sample is positive, and the result of the model identification is also positive.                  
    > **False Negative (FN):** The true class of the sample is positive, but the model recognizes it as negative.                   
    > **False Positive (FP):** The true class of the sample is negative, but the model recognizes it as positive.                  
    > **True Negative (TN):** The true class of the sample is negative, and the model recognizes it as a negative class.      

- ### **Recall:** The ratio of positive values to predicted positive values, also known as the check-all rate, recall shows how much the classifier can predict in an actual positive sample.
    Calculation formula:
    > Recall = TP/(TP+FN)        

- ### **Precision:** Also known as the precision rate, it indicates the proportion of samples that are truly positive in the model's identification of positive classes. In general, the higher the check accuracy rate is, the better the model is.
    Calculation formula:
    > Precision = TP/(TP+FP)         

- ### **Accuracy:** Accuracy is the most commonly used classification performance metric. It can be used to indicate the accuracy of the model, i.e., the number of correctly recognized by the model/total number of samples. In general, the higher the accuracy of the model, the better the model is.
    Calculation formula:
    > Accuracy = (TP+TN)/(TP+FN+FP+TN)


## **How to draw ROC curves?**    
ROC curves can be generated by plotting **True Positive Rate (TPR)** on the y-axis of the graph and **False Positive Rate (FPR)** on the x-axis, which can help us to determine the efficiency of the machine learning model.          
- ### **False Alarm Rate:** Calculated as the ratio of negative samples that the model misidentifies as positive to all negative samples, generally the lower the better.
    Calculation formula:
    > FPR=FP/(TN+FP)
- ### **True Positive Rate:** indicates the ratio of the number of samples that the model recognizes as negative class to the total number of negative class samples.
    Calculation formula:
    > TPR = TP / TP + FN
